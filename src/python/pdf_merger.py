#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„PDFåˆå¹¶æœåŠ¡ç±» - ä¿®å¤æ–‡ä»¶æ’åºæ”¯æŒæ•°å­—ç´¢å¼•
"""

import os
import sys
import json
import logging
import fitz  # PyMuPDF
from datetime import datetime
from urllib.parse import urlparse
from typing import Dict, List, Optional, Callable, Any
import gc
import psutil
import time
import traceback

class PDFMergerError(Exception):
    """PDFåˆå¹¶ç›¸å…³å¼‚å¸¸"""
    pass

class ConfigurationError(PDFMergerError):
    """é…ç½®é”™è¯¯å¼‚å¸¸"""
    pass

class FileProcessingError(PDFMergerError):
    """æ–‡ä»¶å¤„ç†å¼‚å¸¸"""
    pass

class PDFMerger:
    """
    ä¼ä¸šçº§PDFåˆå¹¶æœåŠ¡ç±» - æ™ºèƒ½æ’åºç‰ˆæœ¬

    ç‰¹æ€§ï¼š
    - æ™ºèƒ½æ–‡ä»¶æ’åºï¼ˆæ”¯æŒæ•°å­—ç´¢å¼•å’Œå“ˆå¸Œå‰ç¼€ï¼‰
    - æµå¼å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º
    - å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
    - è¿›åº¦è·Ÿè¸ªå’Œæ€§èƒ½ç›‘æ§
    """

    def __init__(self, config_path: str = 'config.json', logger: Optional[logging.Logger] = None):
        """
        åˆå§‹åŒ–PDFåˆå¹¶å™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            logger: å¯é€‰çš„æ—¥å¿—è®°å½•å™¨
        """
        self.config_path = config_path
        self.logger = logger or self._setup_logger()

        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)

        # è®¾ç½®è·¯å¾„
        self.pdf_dir = self.config['pdfDir']
        self.metadata_dir = os.path.join(
            self.pdf_dir,
            self.config.get('metadata', {}).get('directory', 'metadata')
        )
        self.final_pdf_dir = os.path.join(
            self.pdf_dir,
            self.config.get('output', {}).get('finalPdfDirectory', 'finalPdf')
        )

        # æ€§èƒ½ç›‘æ§
        self.stats = {
            'files_processed': 0,
            'total_pages': 0,
            'start_time': None,
            'memory_peak': 0,
            'errors': []
        }

        # åŠ è½½æ–‡ç« æ ‡é¢˜
        self.article_titles = self._load_article_titles()

        self.logger.info(f"PDFåˆå¹¶å™¨åˆå§‹åŒ–å®Œæˆ - PDFç›®å½•: {self.pdf_dir}")

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®é»˜è®¤æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('PDFMerger')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if not os.path.exists(config_path):
                raise ConfigurationError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            # éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
            required_keys = ['rootURL', 'pdfDir']
            for key in required_keys:
                if key not in config:
                    raise ConfigurationError(f"ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹: {key}")

            return config

        except json.JSONDecodeError as e:
            raise ConfigurationError(f"é…ç½®æ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {e}")
        except Exception as e:
            raise ConfigurationError(f"é…ç½®åŠ è½½å¤±è´¥: {e}")

    def _load_article_titles(self) -> Dict[str, str]:
        """åŠ è½½æ–‡ç« æ ‡é¢˜æ˜ å°„"""
        article_titles = {}

        try:
            # å°è¯•ä»å…ƒæ•°æ®ç›®å½•åŠ è½½
            metadata_file = os.path.join(self.metadata_dir, 'articleTitles.json')
            if os.path.exists(metadata_file):
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    article_titles = json.load(f)
                    self.logger.info(f"ä»å…ƒæ•°æ®ç›®å½•åŠ è½½äº† {len(article_titles)} ä¸ªæ–‡ç« æ ‡é¢˜")

            # å›é€€åˆ°PDFç›®å½•
            if not article_titles:
                fallback_file = os.path.join(self.pdf_dir, 'articleTitles.json')
                if os.path.exists(fallback_file):
                    with open(fallback_file, 'r', encoding='utf-8') as f:
                        article_titles = json.load(f)
                        self.logger.info(f"ä»PDFç›®å½•åŠ è½½äº† {len(article_titles)} ä¸ªæ–‡ç« æ ‡é¢˜")

        except Exception as e:
            self.logger.warning(f"åŠ è½½æ–‡ç« æ ‡é¢˜å¤±è´¥: {e}")

        return article_titles

    def _get_pdf_files(self, directory_path: str, engine_filter: str = None) -> List[str]:
        """
        è·å–ç›®å½•ä¸­çš„PDFæ–‡ä»¶åˆ—è¡¨ï¼ˆæ™ºèƒ½æ’åºï¼‰

        æ”¯æŒï¼š
        1. æ•°å­—å‰ç¼€æ–‡ä»¶ï¼ˆ000-xxx.pdf, 001-xxx.pdfï¼‰- æŒ‰æ•°å­—æ’åº
        2. å“ˆå¸Œå‰ç¼€æ–‡ä»¶ï¼ˆ676cb9dd-xxx.pdfï¼‰- æŒ‰æ–‡ä»¶åˆ›å»ºæ—¶é—´æ’åº
        3. æ··åˆæƒ…å†µ - æ•°å­—æ–‡ä»¶ä¼˜å…ˆï¼Œç„¶åå“ˆå¸Œæ–‡ä»¶
        4. å¼•æ“è¿‡æ»¤ï¼šåªè·å–ç‰¹å®šå¼•æ“ç”Ÿæˆçš„PDFæ–‡ä»¶

        Args:
            directory_path: ç›®å½•è·¯å¾„
            engine_filter: å¼•æ“è¿‡æ»¤å™¨ï¼Œå¯é€‰å€¼ï¼š'puppeteer', 'pandoc', None(æ‰€æœ‰æ–‡ä»¶)
        """
        try:
            if not os.path.exists(directory_path):
                self.logger.warning(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
                return []

            all_files = os.listdir(directory_path)
            self.logger.debug(f"ç›®å½• {directory_path} ä¸­çš„æ‰€æœ‰æ–‡ä»¶: {all_files}")

            files = [
                f for f in all_files
                if f.endswith('.pdf') and os.path.isfile(os.path.join(directory_path, f))
            ]

            # ğŸ”¥ æ–°å¢ï¼šæ ¹æ®å¼•æ“è¿‡æ»¤PDFæ–‡ä»¶
            if engine_filter:
                if engine_filter == 'puppeteer':
                    # åªè¦åŒ…å«_puppeteerçš„æ–‡ä»¶
                    files = [f for f in files if '_puppeteer.pdf' in f]
                elif engine_filter == 'pandoc':
                    # åªè¦åŒ…å«_pandocçš„æ–‡ä»¶
                    files = [f for f in files if '_pandoc.pdf' in f]
                elif engine_filter == 'single':
                    # åªè¦ä¸åŒ…å«_puppeteerå’Œ_pandocçš„æ–‡ä»¶ï¼ˆå•å¼•æ“æ¨¡å¼çš„æ–‡ä»¶ï¼‰
                    files = [f for f in files if '_puppeteer.pdf' not in f and '_pandoc.pdf' not in f]

            if not files:
                return []

            self.logger.debug(f"è¿‡æ»¤åçš„PDFæ–‡ä»¶ (engine_filter={engine_filter}): {files}")

            # ğŸ”¥ æ™ºèƒ½æ’åºé€»è¾‘ï¼šæ”¯æŒæ•°å­—å‰ç¼€å’Œå“ˆå¸Œå‰ç¼€
            def get_sort_key(filename: str) -> tuple:
                try:
                    # å¯¹äºåŒå¼•æ“æ–‡ä»¶ï¼Œéœ€è¦å»æ‰_puppeteeræˆ–_pandocåç¼€æ¥è·å–å‰ç¼€
                    name_for_sorting = filename
                    if '_puppeteer.pdf' in filename:
                        name_for_sorting = filename.replace('_puppeteer.pdf', '.pdf')
                    elif '_pandoc.pdf' in filename:
                        name_for_sorting = filename.replace('_pandoc.pdf', '.pdf')

                    parts = name_for_sorting.split('-', 1)  # åªåˆ†å‰²ç¬¬ä¸€ä¸ªè¿å­—ç¬¦
                    if len(parts) == 0:
                        return (999999, 0, filename)

                    prefix = parts[0]

                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å­—å‰ç¼€ï¼ˆåŒ…æ‹¬è¡¥é›¶çš„æƒ…å†µï¼‰
                    if prefix.isdigit():
                        # æ•°å­—å‰ç¼€ï¼ŒæŒ‰æ•°å­—å¤§å°æ’åºï¼Œä¼˜å…ˆçº§æœ€é«˜
                        return (0, int(prefix), filename)

                    # æ£€æŸ¥æ˜¯å¦ä¸ºè¡¥é›¶çš„æ•°å­—å‰ç¼€ï¼ˆå¦‚ 001, 002ï¼‰
                    try:
                        # å»æ‰å‰å¯¼é›¶ï¼Œä½†ä¿ç•™è‡³å°‘ä¸€ä¸ª0
                        num = int(prefix.lstrip('0') or '0')
                        if prefix.startswith('0') and len(prefix) > 1:
                            # è¿™æ˜¯è¡¥é›¶çš„æ•°å­—ï¼Œä¼˜å…ˆçº§æœ€é«˜
                            return (0, num, filename)
                    except ValueError:
                        pass

                    # æ£€æŸ¥æ˜¯å¦ä¸ºå“ˆå¸Œå‰ç¼€ï¼ˆ8ä½åå…­è¿›åˆ¶ï¼‰
                    if len(prefix) == 8 and all(c in '0123456789abcdef' for c in prefix.lower()):
                        # å“ˆå¸Œå‰ç¼€ï¼ŒæŒ‰æ–‡ä»¶åˆ›å»ºæ—¶é—´æ’åºï¼Œä¼˜å…ˆçº§æ¬¡é«˜
                        try:
                            file_path = os.path.join(directory_path, filename)
                            mtime = os.path.getmtime(file_path)
                            return (1, mtime, filename)
                        except:
                            return (1, 0, filename)

                    # å…¶ä»–æƒ…å†µï¼ŒæŒ‰æ–‡ä»¶åå­—æ¯æ’åºï¼Œä¼˜å…ˆçº§æœ€ä½
                    return (2, 0, filename)

                except Exception as e:
                    self.logger.debug(f"æ’åºé”®ç”Ÿæˆå¤±è´¥ {filename}: {e}")
                    return (999999, 0, filename)

            # æŒ‰æ’åºé”®æ’åº
            files.sort(key=get_sort_key)

            # ç»Ÿè®¡ä¸åŒç±»å‹çš„æ–‡ä»¶
            numeric_files = []
            hash_files = []
            other_files = []

            for f in files:
                # å¯¹äºåŒå¼•æ“æ–‡ä»¶ï¼Œéœ€è¦å»æ‰å¼•æ“åç¼€æ¥è·å–å‰ç¼€
                name_for_analysis = f
                if '_puppeteer.pdf' in f:
                    name_for_analysis = f.replace('_puppeteer.pdf', '.pdf')
                elif '_pandoc.pdf' in f:
                    name_for_analysis = f.replace('_pandoc.pdf', '.pdf')

                prefix = name_for_analysis.split('-')[0] if '-' in name_for_analysis else ''
                if prefix.isdigit() or (prefix.startswith('0') and prefix.isdigit()):
                    numeric_files.append(f)
                elif len(prefix) == 8 and all(c in '0123456789abcdef' for c in prefix.lower()):
                    hash_files.append(f)
                else:
                    other_files.append(f)

            # è¾“å‡ºæ’åºä¿¡æ¯
            engine_info = f" ({engine_filter}å¼•æ“)" if engine_filter else ""
            self.logger.info(f"æ‰¾åˆ° {len(files)} ä¸ªPDFæ–‡ä»¶åœ¨ {directory_path}{engine_info}")
            if numeric_files:
                self.logger.info(f"  âœ“ {len(numeric_files)} ä¸ªæ•°å­—å‰ç¼€æ–‡ä»¶ (æŒ‰ç´¢å¼•é¡ºåº)")
            if hash_files:
                self.logger.info(f"  âœ“ {len(hash_files)} ä¸ªå“ˆå¸Œå‰ç¼€æ–‡ä»¶ (æŒ‰æ—¶é—´é¡ºåº)")
            if other_files:
                self.logger.info(f"  âœ“ {len(other_files)} ä¸ªå…¶ä»–æ–‡ä»¶ (æŒ‰åç§°é¡ºåº)")

            self.logger.debug(f"æ’åºåæ–‡ä»¶åˆ—è¡¨å‰5ä¸ª: {files[:5]}")
            return files

        except Exception as e:
            self.logger.error(f"è·å–PDFæ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return []

    def _create_bookmark_title(self, filename: str, article_titles: Dict[str, str]) -> str:
        """
        åˆ›å»ºä¹¦ç­¾æ ‡é¢˜ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        
        ğŸ”§ ä¿®å¤ï¼šåŒå¼•æ“æ¨¡å¼ä¸‹æ­£ç¡®å¤„ç†å¼•æ“åç¼€ï¼Œé¿å…æ ‡é¢˜ä¸­å‡ºç°"Puppeteer"æˆ–"Pandoc"

        ä¼˜å…ˆçº§ï¼š
        1. æ–‡ç« æ ‡é¢˜æ˜ å°„
        2. æ¸…ç†åçš„æ–‡ä»¶åï¼ˆç§»é™¤å¼•æ“åç¼€ï¼‰
        
        æ”¯æŒçš„æ–‡ä»¶åæ ¼å¼ï¼š
        - 001-page-name.pdf â†’ "Page Name"
        - 001-page-name_puppeteer.pdf â†’ "Page Name" (ç§»é™¤å¼•æ“åç¼€)
        - 001-page-name_pandoc.pdf â†’ "Page Name" (ç§»é™¤å¼•æ“åç¼€)
        """
        try:
            self.logger.debug(f"ä¸ºæ–‡ä»¶åˆ›å»ºä¹¦ç­¾æ ‡é¢˜: {filename}")

            # ğŸ”¥ é¦–å…ˆç§»é™¤å¼•æ“åç¼€ï¼ˆ_puppeteer æˆ– _pandocï¼‰
            cleaned_filename = filename
            if '_puppeteer.pdf' in filename:
                cleaned_filename = filename.replace('_puppeteer.pdf', '.pdf')
                self.logger.debug(f"ç§»é™¤Puppeteerå¼•æ“åç¼€: {filename} -> {cleaned_filename}")
            elif '_pandoc.pdf' in filename:
                cleaned_filename = filename.replace('_pandoc.pdf', '.pdf')
                self.logger.debug(f"ç§»é™¤Pandocå¼•æ“åç¼€: {filename} -> {cleaned_filename}")

            # æå–å‰ç¼€å’Œæ–‡ä»¶åéƒ¨åˆ†
            parts = cleaned_filename.split('-', 1)  # åªåˆ†å‰²ç¬¬ä¸€ä¸ªè¿å­—ç¬¦
            if len(parts) < 2:
                title = os.path.splitext(cleaned_filename)[0]
                self.logger.debug(f"æ— å‰ç¼€æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ ‡é¢˜: {title}")
                return title

            prefix = parts[0]
            name_part = parts[1]

            # ğŸ”¥ å°è¯•ä»æ–‡ç« æ ‡é¢˜æ˜ å°„ä¸­æŸ¥æ‰¾
            # æ”¯æŒæ•°å­—å‰ç¼€ï¼ˆåŒ…æ‹¬è¡¥é›¶ï¼‰å’ŒåŸå§‹å‰ç¼€
            possible_keys = [prefix]

            # å¦‚æœæ˜¯æ•°å­—å‰ç¼€ï¼Œæ·»åŠ å¤šç§å¯èƒ½çš„é”®æ ¼å¼
            if prefix.isdigit():
                num = int(prefix)
                possible_keys.extend([
                    str(num),                    # å»æ‰å‰å¯¼é›¶: "1"
                    str(num).zfill(3),          # 3ä½è¡¥é›¶: "001"
                    str(num).zfill(2)           # 2ä½è¡¥é›¶: "01"
                ])

            # æŸ¥æ‰¾æ ‡é¢˜æ˜ å°„
            for key in possible_keys:
                if key in article_titles:
                    title = article_titles[key]
                    self.logger.debug(f"æ‰¾åˆ°æ–‡ç« æ ‡é¢˜æ˜ å°„ {key}: {title}")
                    return title

            # å¦‚æœæ²¡æ‰¾åˆ°æ˜ å°„ï¼Œä½¿ç”¨æ¸…ç†åçš„æ–‡ä»¶å
            cleaned_name = os.path.splitext(name_part)[0]
            # å°†è¿å­—ç¬¦å’Œä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä½¿ç”¨æ ‡é¢˜æ ¼å¼
            title = cleaned_name.replace('-', ' ').replace('_', ' ')
            # è½¬æ¢ä¸ºæ ‡é¢˜æ ¼å¼ï¼šæ¯ä¸ªå•è¯é¦–å­—æ¯å¤§å†™
            title = ' '.join(word.capitalize() for word in title.split())

            self.logger.debug(f"ä½¿ç”¨æ¸…ç†åçš„æ–‡ä»¶åä½œä¸ºæ ‡é¢˜: {title}")
            return title

        except Exception as e:
            self.logger.warning(f"åˆ›å»ºä¹¦ç­¾æ ‡é¢˜å¤±è´¥ {filename}: {e}")
            return os.path.splitext(filename)[0]

    def _monitor_memory(self) -> None:
        """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            self.stats['memory_peak'] = max(self.stats['memory_peak'], memory_mb)

            # å¦‚æœå†…å­˜ä½¿ç”¨è¶…è¿‡é˜ˆå€¼ï¼Œå¼ºåˆ¶åƒåœ¾å›æ”¶
            if memory_mb > 500:  # 500MBé˜ˆå€¼
                gc.collect()
                self.logger.debug(f"å†…å­˜ä½¿ç”¨: {memory_mb:.1f}MB, å·²æ‰§è¡Œåƒåœ¾å›æ”¶")
        except Exception:
            pass  # å†…å­˜ç›‘æ§å¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹

    def merge_pdfs_stream(
        self,
        directory_path: str,
        output_path: str,
        progress_callback: Optional[Callable[[int, int], None]] = None,
        engine_filter: str = None
    ) -> bool:
        """æµå¼åˆå¹¶PDFæ–‡ä»¶"""
        try:
            files = self._get_pdf_files(directory_path, engine_filter)
            if not files:
                self.logger.warning(f"ç›®å½•ä¸­æ²¡æœ‰PDFæ–‡ä»¶: {directory_path}")
                return False

            merged_pdf = None
            current_file_pdf = None

            try:
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(output_path), exist_ok=True)

                merged_pdf = fitz.open()  # åˆ›å»ºç©ºçš„PDFæ–‡æ¡£
                toc = []  # ç›®å½•ç»“æ„

                self.logger.info(f"å¼€å§‹åˆå¹¶ {len(files)} ä¸ªPDFæ–‡ä»¶åˆ° {output_path}")

                for i, filename in enumerate(files):
                    try:
                        self.logger.debug(f"å¤„ç†æ–‡ä»¶ {i+1}/{len(files)}: {filename}")
                        file_path = os.path.join(directory_path, filename)

                        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                        if not os.path.exists(file_path):
                            self.logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                            continue

                        # æ‰“å¼€å½“å‰PDFæ–‡ä»¶
                        current_file_pdf = fitz.open(file_path)
                        page_count = current_file_pdf.page_count

                        if page_count == 0:
                            self.logger.warning(f"è·³è¿‡ç©ºPDFæ–‡ä»¶: {filename}")
                            current_file_pdf.close()
                            continue

                        # è®°å½•åˆå¹¶å‰çš„é¡µæ•°
                        start_page = merged_pdf.page_count

                        # æ’å…¥PDFé¡µé¢
                        merged_pdf.insert_pdf(current_file_pdf)

                        # åˆ›å»ºä¹¦ç­¾
                        bookmark_title = self._create_bookmark_title(filename, self.article_titles)
                        toc.append([
                            1,  # çº§åˆ«
                            bookmark_title,  # æ ‡é¢˜
                            start_page + 1,  # é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
                            {"kind": 1, "page": start_page}  # é“¾æ¥ä¿¡æ¯
                        ])

                        # å…³é—­å½“å‰æ–‡ä»¶
                        current_file_pdf.close()
                        current_file_pdf = None

                        # æ›´æ–°ç»Ÿè®¡
                        self.stats['files_processed'] += 1
                        self.stats['total_pages'] += page_count

                        # å†…å­˜ç›‘æ§
                        self._monitor_memory()

                        # è¿›åº¦å›è°ƒ
                        if progress_callback:
                            progress_callback(i + 1, len(files))

                        self.logger.debug(f"å·²åˆå¹¶: {filename} ({page_count} é¡µ) -> ä¹¦ç­¾: {bookmark_title}")

                    except Exception as e:
                        error_msg = f"å¤„ç†æ–‡ä»¶å¤±è´¥ {filename}: {e}"
                        self.logger.error(error_msg)
                        self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                        self.stats['errors'].append(error_msg)

                        if current_file_pdf:
                            current_file_pdf.close()
                            current_file_pdf = None

                        # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶
                        continue

                # è®¾ç½®ç›®å½•ç»“æ„
                if toc:
                    merged_pdf.set_toc(toc)
                    self.logger.info(f"è®¾ç½®äº† {len(toc)} ä¸ªä¹¦ç­¾")

                # ä¿å­˜åˆå¹¶åçš„PDF
                merged_pdf.save(output_path)
                self.logger.info(f"PDFåˆå¹¶å®Œæˆ: {output_path}")

                return True

            except Exception as e:
                error_msg = f"PDFåˆå¹¶å¤±è´¥: {e}"
                self.logger.error(error_msg)
                self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                self.stats['errors'].append(error_msg)
                raise FileProcessingError(error_msg)

            finally:
                # æ¸…ç†èµ„æº
                if current_file_pdf:
                    current_file_pdf.close()
                if merged_pdf:
                    merged_pdf.close()

                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()

        except Exception as e:
            self.logger.error(f"merge_pdfs_stream æ‰§è¡Œå¤±è´¥: {e}")
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return False

    def _detect_dual_engine_mode(self, directory_path: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºåŒå¼•æ“æ¨¡å¼ï¼ˆåŒ…å«_puppeteerå’Œ_pandocæ–‡ä»¶ï¼‰"""
        try:
            files = os.listdir(directory_path)
            pdf_files = [f for f in files if f.endswith('.pdf')]
            
            has_puppeteer = any('_puppeteer.pdf' in f for f in pdf_files)
            has_pandoc = any('_pandoc.pdf' in f for f in pdf_files)
            
            return has_puppeteer and has_pandoc
        except Exception:
            return False

    def merge_directory(self, directory_name: Optional[str] = None) -> List[str]:
        """åˆå¹¶æŒ‡å®šç›®å½•æˆ–æ‰€æœ‰å­ç›®å½•çš„PDFæ–‡ä»¶"""
        try:
            if not os.path.exists(self.pdf_dir):
                raise FileProcessingError(f"PDFç›®å½•ä¸å­˜åœ¨: {self.pdf_dir}")

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(self.final_pdf_dir, exist_ok=True)

            # è·å–åŸŸåå’Œæ—¥æœŸ
            url = urlparse(self.config['rootURL'])
            domain = url.hostname.replace('.', '_') if url.hostname else 'unknown'
            current_date = datetime.now().strftime('%Y%m%d')

            merged_files = []

            if directory_name:
                # åˆå¹¶æŒ‡å®šç›®å½•
                directory_path = os.path.join(self.pdf_dir, directory_name)
                if os.path.isdir(directory_path):
                    # æ£€æµ‹æ˜¯å¦ä¸ºåŒå¼•æ“æ¨¡å¼
                    is_dual_engine = self._detect_dual_engine_mode(directory_path)
                    
                    if is_dual_engine:
                        # åŒå¼•æ“æ¨¡å¼ï¼šåˆ†åˆ«åˆå¹¶ä¸¤ç§ç±»å‹çš„PDF
                        self.logger.info(f"æ£€æµ‹åˆ°åŒå¼•æ“æ¨¡å¼ï¼Œå°†ç”Ÿæˆä¸¤ä¸ªåˆå¹¶PDFæ–‡ä»¶")
                        
                        # åˆå¹¶Puppeteerç‰ˆæœ¬
                        puppeteer_output = os.path.join(
                            self.final_pdf_dir,
                            f"{directory_name}_puppeteer_{current_date}.pdf"
                        )
                        if self.merge_pdfs_stream(directory_path, puppeteer_output, engine_filter='puppeteer'):
                            merged_files.append(puppeteer_output)
                            
                        # åˆå¹¶Pandocç‰ˆæœ¬
                        pandoc_output = os.path.join(
                            self.final_pdf_dir,
                            f"{directory_name}_pandoc_{current_date}.pdf"
                        )
                        if self.merge_pdfs_stream(directory_path, pandoc_output, engine_filter='pandoc'):
                            merged_files.append(pandoc_output)
                    else:
                        # å•å¼•æ“æ¨¡å¼ï¼šæ­£å¸¸åˆå¹¶
                        output_path = os.path.join(
                            self.final_pdf_dir,
                            f"{directory_name}_{current_date}.pdf"
                        )
                        if self.merge_pdfs_stream(directory_path, output_path):
                            merged_files.append(output_path)
                else:
                    self.logger.warning(f"æŒ‡å®šç›®å½•ä¸å­˜åœ¨: {directory_path}")
            else:
                # é¦–å…ˆåˆå¹¶æ ¹ç›®å½•
                is_dual_engine = self._detect_dual_engine_mode(self.pdf_dir)
                
                if is_dual_engine:
                    # åŒå¼•æ“æ¨¡å¼ï¼šåˆ†åˆ«åˆå¹¶ä¸¤ç§ç±»å‹çš„PDF
                    self.logger.info(f"æ£€æµ‹åˆ°åŒå¼•æ“æ¨¡å¼ï¼Œå°†ç”Ÿæˆä¸¤ä¸ªåˆå¹¶PDFæ–‡ä»¶")
                    
                    # åˆå¹¶Puppeteerç‰ˆæœ¬
                    puppeteer_output = os.path.join(
                        self.final_pdf_dir,
                        f"{domain}_puppeteer_{current_date}.pdf"
                    )
                    if self.merge_pdfs_stream(self.pdf_dir, puppeteer_output, engine_filter='puppeteer'):
                        merged_files.append(puppeteer_output)
                        
                    # åˆå¹¶Pandocç‰ˆæœ¬
                    pandoc_output = os.path.join(
                        self.final_pdf_dir,
                        f"{domain}_pandoc_{current_date}.pdf"
                    )
                    if self.merge_pdfs_stream(self.pdf_dir, pandoc_output, engine_filter='pandoc'):
                        merged_files.append(pandoc_output)
                else:
                    # å•å¼•æ“æ¨¡å¼ï¼šæ­£å¸¸åˆå¹¶
                    root_output_path = os.path.join(
                        self.final_pdf_dir,
                        f"{domain}_{current_date}.pdf"
                    )
                    if self.merge_pdfs_stream(self.pdf_dir, root_output_path):
                        merged_files.append(root_output_path)

                # ç„¶ååˆå¹¶æ‰€æœ‰å­ç›®å½•
                try:
                    items = os.listdir(self.pdf_dir)
                    self.logger.debug(f"PDFç›®å½•ä¸­çš„æ‰€æœ‰é¡¹ç›®: {items}")

                    for item in items:
                        try:
                            item_path = os.path.join(self.pdf_dir, item)

                            # è·³è¿‡éç›®å½•å’Œç‰¹æ®Šç›®å½•
                            if not os.path.isdir(item_path) or item in ['finalPdf', 'metadata', '.temp']:
                                self.logger.debug(f"è·³è¿‡é¡¹ç›®: {item} (éç›®å½•æˆ–ç‰¹æ®Šç›®å½•)")
                                continue

                            self.logger.info(f"å¤„ç†å­ç›®å½•: {item}")
                            
                            # æ£€æµ‹å­ç›®å½•æ˜¯å¦ä¸ºåŒå¼•æ“æ¨¡å¼
                            is_dual_engine_subdir = self._detect_dual_engine_mode(item_path)
                            
                            if is_dual_engine_subdir:
                                # åŒå¼•æ“æ¨¡å¼ï¼šåˆ†åˆ«åˆå¹¶ä¸¤ç§ç±»å‹çš„PDF
                                # åˆå¹¶Puppeteerç‰ˆæœ¬
                                puppeteer_output = os.path.join(
                                    self.final_pdf_dir,
                                    f"{item}_puppeteer_{current_date}.pdf"
                                )
                                if self.merge_pdfs_stream(item_path, puppeteer_output, engine_filter='puppeteer'):
                                    merged_files.append(puppeteer_output)
                                    
                                # åˆå¹¶Pandocç‰ˆæœ¬
                                pandoc_output = os.path.join(
                                    self.final_pdf_dir,
                                    f"{item}_pandoc_{current_date}.pdf"
                                )
                                if self.merge_pdfs_stream(item_path, pandoc_output, engine_filter='pandoc'):
                                    merged_files.append(pandoc_output)
                            else:
                                # å•å¼•æ“æ¨¡å¼ï¼šæ­£å¸¸åˆå¹¶
                                output_path = os.path.join(
                                    self.final_pdf_dir,
                                    f"{item}_{current_date}.pdf"
                                )
                                if self.merge_pdfs_stream(item_path, output_path):
                                    merged_files.append(output_path)

                        except Exception as e:
                            self.logger.error(f"å¤„ç†å­ç›®å½• {item} æ—¶å‡ºé”™: {e}")
                            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                            continue

                except Exception as e:
                    self.logger.error(f"åˆ—å‡ºPDFç›®å½•å†…å®¹æ—¶å‡ºé”™: {e}")
                    self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

            return merged_files

        except Exception as e:
            error_msg = f"ç›®å½•åˆå¹¶å¤±è´¥: {e}"
            self.logger.error(error_msg)
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise FileProcessingError(error_msg)

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–åˆå¹¶ç»Ÿè®¡ä¿¡æ¯"""
        elapsed_time = 0
        if self.stats['start_time']:
            elapsed_time = time.time() - self.stats['start_time']

        return {
            'files_processed': self.stats['files_processed'],
            'total_pages': self.stats['total_pages'],
            'elapsed_time': elapsed_time,
            'memory_peak_mb': self.stats['memory_peak'],
            'errors_count': len(self.stats['errors']),
            'errors': self.stats['errors'][-10:],  # æœ€è¿‘10ä¸ªé”™è¯¯
            'avg_pages_per_file': (
                self.stats['total_pages'] / self.stats['files_processed']
                if self.stats['files_processed'] > 0 else 0
            ),
            'processing_speed': (
                self.stats['files_processed'] / elapsed_time
                if elapsed_time > 0 else 0
            )
        }

    def run(self) -> Dict[str, Any]:
        """è¿è¡ŒPDFåˆå¹¶ä»»åŠ¡"""
        self.stats['start_time'] = time.time()
        self.logger.info("å¼€å§‹PDFåˆå¹¶ä»»åŠ¡ï¼ˆæ™ºèƒ½æ’åºç‰ˆæœ¬ï¼‰")

        try:
            # æ‰§è¡Œåˆå¹¶
            merged_files = self.merge_directory()

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.get_statistics()

            result = {
                'success': True,
                'merged_files': merged_files,
                'statistics': stats
            }

            self.logger.info(f"PDFåˆå¹¶ä»»åŠ¡å®Œæˆ: å¤„ç†äº† {stats['files_processed']} ä¸ªæ–‡ä»¶, "
                           f"å…± {stats['total_pages']} é¡µ, "
                           f"ç”¨æ—¶ {stats['elapsed_time']:.1f} ç§’")

            return result

        except Exception as e:
            error_msg = f"PDFåˆå¹¶ä»»åŠ¡å¤±è´¥: {e}"
            self.logger.error(error_msg)
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

            return {
                'success': False,
                'error': error_msg,
                'statistics': self.get_statistics()
            }

def main():
    """ä¸»å‡½æ•°ï¼Œæ”¯æŒå‘½ä»¤è¡Œæ‰§è¡Œ"""
    import sys
    import argparse

    parser = argparse.ArgumentParser(description='æ™ºèƒ½PDFåˆå¹¶å·¥å…·')
    parser.add_argument('--config', default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--directory', help='æŒ‡å®šè¦åˆå¹¶çš„ç›®å½•å')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)

    try:
        # åˆ›å»ºPDFåˆå¹¶å™¨
        merger = PDFMerger(config_path=args.config)

        # æ‰§è¡Œåˆå¹¶
        if args.directory:
            merged_files = merger.merge_directory(args.directory)
        else:
            result = merger.run()
            merged_files = result.get('merged_files', [])

        # è¾“å‡ºç»“æœ
        print(f"\nâœ… åˆå¹¶å®Œæˆ! ç”Ÿæˆäº† {len(merged_files)} ä¸ªPDFæ–‡ä»¶:")
        for file_path in merged_files:
            print(f"  ğŸ“„ {file_path}")

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        stats = merger.get_statistics()
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - å¤„ç†æ–‡ä»¶æ•°: {stats['files_processed']}")
        print(f"  - æ€»é¡µæ•°: {stats['total_pages']}")
        print(f"  - ç”¨æ—¶: {stats['elapsed_time']:.1f} ç§’")
        print(f"  - å†…å­˜å³°å€¼: {stats['memory_peak_mb']:.1f} MB")

        if stats['errors_count'] > 0:
            print(f"  âš ï¸  é”™è¯¯æ•°: {stats['errors_count']}")

        return 0

    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}", file=sys.stderr)
        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}", file=sys.stderr)
        return 1

if __name__ == '__main__':
    sys.exit(main())
